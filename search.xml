<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hexo+next博客搭建]]></title>
    <url>%2Fblog%2F2019%2F08%2F16%2Fhexo-next%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA1%2F</url>
    <content type="text"><![CDATA[前言在一天逛B站的时候，看到了羊哥（codesheep）发布的“手把手教你从0开始搭建自己的个人博客 |无坑版视频教程| hexo”视频，链接：https://www.bilibili.com/video/av44544186?t=583 。就根据视频进行了博客系统的搭建。 经过查阅其他的人的博客系统以及自己的摸索，自己使用hexo+next来搭建。也可以根据Next官方文档配置（https://theme-next.org/docs/getting-started/） 安装nodejs 和 git windows10可以直接在nodejs官方下载，以及git工具。如图 Ubuntu18.04可以直接sudo apt install nodejs来下载(ubuntu配置博客功能时有问题，我没时间再搞，以下主要是win10平台) 安装好之后，新建一个文件夹专门用来存放自己的blog系统，这样即使自己搞坏了博客系统配置，也可以删除该文件夹，重新配置blog，丝毫不会影响电脑中的其他文件。例如我新建了E:/MyBlog文件，进入文件右键点击 git bash here, 进入控制台，可以输入node -v和npm -v来查看node版本，可以时常通过pwd来确定自己所在路径，确保在MyBlog目录下。 更换淘宝源直接采用npm下载很慢，我们通过换用淘宝源来下载hexo代码： 1npm install -g cnpm --registry=https://registry.npm.taobao.org 安装完成可利用cnpm -v查看版本号输入cnpm install -g hexo-cli来安装hexo，完成之后hexo -v检查版本号 初始化blog系统在MyBlog文件夹下输入hexo init来初始化博客系统。 遇到的问题：git clone特别慢 解决办法：修改hosts文件。windows下：用编辑器打开host文件： C:\Windows\System32\drivers\etc\hosts， 在hosts文件末尾添加下面两个地址; Ubuntu下同理，sudo gedit /etc/hosts打开hosts，添加下面两个地址，sudo /etc/init.d/networking restart重新载入hosts12151.101.72.249 github.global.ssl.fastly.net192.30.253.112 github.com 从而初始化得到博客系统。以下一些在git bash中操作指令123hexo clean #清理hexo中生成的博客文件，用来初始化一下hexo g #生成博客文件hexo s #开启博客服务，本地可以在`localhost:4000`来访问，`ctrl+c`终止博客服务。 特别提醒：Ubuntu尽可能在管理员下操作，sudo su从而保证权限。 配置主题在MyBlog文件夹下的git bash输入git clone https://github.com/theme-next/hexo-theme-next themes/next，从而clone下来next主题文件。我们的优化都是基于next主题进行的。首先在站点配置文件MyBlog/_config.yml设置，可以利用nodepad++或者其他开发工具打开该配置文件，进行如下设置 然后配置主题配置themes/next/_config.yml: 修改站点图标 可以自己找的图像ps成16*16，32*32各1张，放到路径themes/next/source/images下。特别注意涉及到主题一些图像的变换，一般都是放到主题下面的这个文件夹中，在配置文件的路径填写时下/images/图像名即可 个人链接 个人的一些社交媒体的账号展示 个人头像 同理，将自己的头像放到next/source/images下. 主页显示的标识 主页显示的标识。同时，next下有四个主题，在172到176行可以选择，我选择的第四个，所以直接把注释#去掉即可。 效果展示 此时点击关于、标签、归类时，显示是空界面，因为我们没创建。 bash或者git bash执行：hexo new page categories，在MyBlog的文件source就会多出来一个categories文件夹，打开该文件夹，其中的index.md写入： 12345---title: 分类date: 2019-08-16 18:11:08type: &quot;categories&quot;--- 三个短线表示md文件的标记，其中日期不用管，主要更改title，以及type，将其值设置为”categories”，注意，冒号之后都要有空格。 同样方法更改bags：hexo new page tags，type设置为tags。 特别注意：此后写的博客中的标签和分类都会自动归档到分类和标签中。但是要在博客的md文件中声明分类和标签。每次都去写声明很麻烦，我们可以在MyBlog/scaffolds/post.md中设置好模板。输入： 123456---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:categories:--- 写第一篇博客在MgBlog文件夹下的git bash 运行hexo n 博客名 我们的博客的md文件其实都存放在Myblog/source/_posts中，我们在刚生成的md文件中书写博客，运行hexo g和hexo s就可以在本地的浏览器localhost:4000中查看。如果我们不想要这篇博客了，直接删除该文件夹中该博客的.md文件即可。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>nodejs</tag>
        <tag>gitclone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word Embedding介绍]]></title>
    <url>%2Fblog%2F2019%2F08%2F15%2Fembedding%2F</url>
    <content type="text"><![CDATA[文本特征选择就是在训练集中选择一个词汇子集的过程。 现在的方法不能直接处理文本数据，通过Word Embedding的方法将文本数据转化为数值型数据。如果将word看作文本的最小单元，可以将Word Embedding理解为一种映射，或者说是嵌入（embedding）到一个数值向量空间。之所以称之为Embedding，因为这种方法往往伴随着一种降维的思想。 就是把one-hot编码的词嵌入到一个低维空间。 常用的文本特征选择方法有 词频法 TF 词频逆文档频率法 TF-IDF 信息增益 IG 互信息 MI TF 和 IDF都是在不考虑分类分布的情况下消除低频词 Word Embedding的输入Word Embedding的输入是原始文本中一组不重叠的词汇，假设有句子：apple on a apple tree 为了方便处理，把这些词汇放在一个dictionary：[apple, on, a, tree] ，这个dictionary可以看做wordEmbedding的输入 Word Embedding的输出就是每一word的向量表示，最简单的方式就是one-hot编码方式，那么每一word都对应了一种数值表示。 apple对应的vector就是[1,0,0,0] a对应的是[0,0,1,0] Word Embedding的类型主要有两种： 基于频率 基于预测 基于频率的Word Embedding基于频率的Word Embedding又可以分为： Count Vector TF—IDF Vector Co-Occurence Vector 其本质都是基于one-hot，以频率为主的加权。 Count Vector假设有一个语料库C，其中有D个文档：$d_1,d_2,\dots,d_D$,$C$中一共有$N$个word，这$N$个word构成原始输入的dictionary，据此可以生成一个矩阵M，其规模为$D\times N$ 例子： 语料库内容如下： D1：He is a boy D2：She is a girl，good girl 那么可以构造一个$2\times 7$的矩阵 He She is a boy girl good D1 1 0 1 1 1 0 0 D2 0 1 1 1 0 2 1 每个文档用词向量来表示，那么dictionary很庞大，矩阵是稀疏的，大量的无用信息。通常的做法是选取出现次数最频繁的那些词来构建dictionary，例如top10000 TF-IDF上面的只考虑到了词频TF，也就是词在文档中出现的频率。TF越大，说明词在本文档中的重要性越高，对应的权重也就越高。这个思路大体上来说是对的，例如，对于一个主题是Cat的文档，显然Cat这个词汇在本文档中的出现频率会相对高。 但如果我们把视野扩展到整个语料库，会发现，像is，a等通用词汇，几乎在每个文档里出现的频率都很高。由此，我们可以得到这样的结论：对于一个word，如果在特定文档里出现的频率高，而在整个语料库里出现的频率低，那么这个word对于该文档的重要性就比较高。因此我们可以引入逆文档频率IDF（Inverse Document Frequency）的概念： ​ $IDF = log(N/n)$ N代表语料库中文档的总数，n代表某个word在n个文档中出现过。 因此当一个word出现的频繁，那么IDF就越小。IDF用来惩罚常用词汇，TF用来奖励那么特定文档中出现频繁的词汇。用$TF \times IDF$来表示词汇的权重，这样可以提取出来文档的关键词。 对于一个word， 如果在特定文档里出现的频率高，即TF较大，则反映出了该word在自己的文档中可能是比较关键的词。 在整个语料库里出现的频率低，即IDF较小，则排除了常用词的干扰。 例子： 语料库共有2个文档，其中一个文档名为d，且文档d一共有8个词汇。 cat只出现在文档d中，出现过4次； is在两个文档中都有出现，在d中出现了4次。 那么对于文档d进行提取关键词： $TF(‘cat’, d) = \frac{4}{8} = 0.5$ $TF(‘is’, d) = \frac{4}{8} = 0.5$ $IDF(‘cat’, d) = log(\frac21) = 0.301$ $IDF(‘is’, d) = log(\frac22) = 0$ 那么计算TF-IDF $TFIDF(‘cat’, d) = 0.5 \times 0.301 = 0.15$ $TFIDF(‘is’, d) = 0$ 这样就能找出来文件的关键词汇。 Co-Occurence Vector自然语言一大特色是语义和上下文。有如下著名的研究结果：相似的单词趋向于有相似的上下文(context)。举例： 那个人是个男孩 那个人是个女孩 Context Window 上下文窗口，需要定义其长度。例如： 定义窗口大小为2，则such的Context Window如图所示。 Co-Occurence（共现） 对于such这个单词来说，在其上下文窗口内，它分别与[she, is, a, beautiful]这四个单词各出现了一次共现。如果我们在语料库中所有such出现的地方，计算其共现的单词，并按次数累加，那么我们就可以利用其上下文范围内的单词来表示such这个词，这就是Co-Occurence Vector的计算方法。 假定有如下语料库： He is not lazy. He is intelligent. He is smart. 当窗口大小为2的时候，可以得到如下共现矩阵： 计算过程如图所示： 这种方法可以保留语义信息。 基于预测的Word Embedding算法满足 携带上下文信息 词的表示是稠密的 算法主要有CBOW和Skip-Gram CBOW （continues bag of words）本质是通过context来预测word 如图所示： 首先语料库内的每一word都可以用one-hot编码，假设选取context window为2，那么模型中的一对input和target就是： input：He 和 is 的one-hot编码 target：a 的 one-hot编码 算法步骤： 窗口大小为C/2，W：$V\times N$， $V =( \sum_{i=0}^{C} W\times C_i)/C$ 最终网络输出层才用softmax，即计算概率，与a 的one-hot编码作对比。该方法关注点在于hidden layer的输出，因此word是one-hot编码， 1234import tensorflow as tfhello = tf.constance('Hello Google!')sess = tf.Session()print(sess.run(hello))]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2019%2F08%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
