<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ubuntu配置远程SSH]]></title>
    <url>%2Fblog%2F2019%2F08%2F30%2Fubuntu%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8BSSH%2F</url>
    <content type="text"><![CDATA[前言：在Ubuntu系统中配置了openssh，这样可以远程访问台式机的jupyter notebook和terminator，但是只能局域网使用，查看资料使用花生壳的个人测试版（免费使用，两个内网穿透映射）进行内网穿透，从而达到在外网使用putty访问或者直接访问jupyter notebook 安装openssh1sudo apt install openssh-server 这时通过ifconfig就可以查看到本机在内网中的ip地址。 设置jupyter notebook 生成配置文件1jupyter notebook --generate-config 此时会得到配置文件，其默认路径一般如下：~/.jupyter/jupyter_notebook_config.py 设置登录密码 123$jupyter notebook passwordEnter password: yourcode #输入密码Verify password: yourcodeagain #再次输入密码确认 修改配置文件 1234#把前面的#去掉c.NotebookApp.ip = &apos;*&apos; #允许所有ip访问 补充:报错 No address associated with hostname可设置为:&apos;0.0.0.0&apos;c.NotebookApp.open_browser = False #不打开浏览器c.NotebookApp.port = 8888 #端口为8888 这样，我们在内网中的电脑就可以访问开启jupyternotebook服务的台式机，在浏览器中输入之前ifconfig查到的ip加上:端口号（我们设置的是:8888)就可以直接访问jupyternotebook，此外也可以用putty进行ssh的终端进行访问 内网穿透我采用了花生壳的个人服务，首先是下载花生壳的Ubuntu版本，其次安装时进入管理员模式12sudo sudpkg -i phddns----.deb 安装之后会得到sn秘钥，一定要记录下这个秘钥，密码默认时admin之后开启服务1phddns start 注册花生壳时会得到一个免费域名 点击内网穿透 新建内网穿透 新键一个内网穿透进行设置端口映射之后可以进行外网访问，不过就是流量有点少，就只有1个G，用来应对有时候teamviewer连接不到服务器的情况吧。参考JupyterNotebook配置远程登录花生壳关于Linux下的安装说明]]></content>
      <categories>
        <category>环境安装</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu18.04安装和设置深度终端]]></title>
    <url>%2Fblog%2F2019%2F08%2F24%2Fubuntu18-04%E5%AE%89%E8%A3%85%E5%92%8C%E8%AE%BE%E7%BD%AE%E6%B7%B1%E5%BA%A6%E7%BB%88%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[前言：Ubuntu18.04的终端太难用，有比较好的选择是terminator和深度终端。安装了terminator后，利用快捷键Ctrl+Alt+t，打开的就是terminator，Ubuntu已经把默认的打开变成了terminator，但是右键在文件打开依旧是Ubuntu的自带终端。所以设置右键在文件中打开terminator或者深度终端（deepin） 安装terminator和深度终端安装方式还是比较简单的。对于terminator来说，可以直接在终端中输入1sudo apt install terminator 对于深度终端，可以直接在Ubuntu的商店中，搜索深度终端，就可以安装使用了非常方便，deepin是国内很优秀的Linux操作系统，点赞！我使用深度终端，但是terminator配置右键的方法和深度终端是一样的。 安装Nautilus Actions我查到的文件管理器扩展，但是由于Ubuntu不在维护这个扩展，现在不能直接下载，它已被重命名为FileManager actions。安装步骤： 添加软件源： 1sudo add-apt-repository ppa:daniel-marynicz/filemanager-actions 安装扩展插件： 12sudo apt updatesudo apt install filemanager-actions-nautilus-extension 安装完成就可以在启动程序界面找到FileManager Actions卸载方式：1sudo apt remove --autoremove filemanager-actions 设置右键在文件夹中启动深度终端 打开FileManager Actions软件 首先点文件-新建动作 在动作选项卡中进行如下设置 在命令选项卡中进行如下设置其中：路径：/usr/bin/deepin-terminal参数：--working-directory=%d/%b工作目录：%d/%b 在配置将菜单布局中第一项取消勾选重启电脑之后即设置完成演示：由于在桌面打开深度终端，默认在/目录下，深度终端中设置了快捷键，直接cd 到用户目录。参考：https://www.linuxidc.com/Linux/2019-01/156228.htmhttps://blog.csdn.net/zhanghm1995/article/details/89419109]]></content>
      <categories>
        <category>环境安装</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>深度终端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像分类实践1总结]]></title>
    <url>%2Fblog%2F2019%2F08%2F22%2F%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%AE%9E%E8%B7%B51%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[前言 总结自己在做一个小的图像分类时的经验，采用的框架：pytroch，基于一个只有２００张图像的农业害虫数据集，害虫种类有１０类，数据集的质量较差。 数据集小导致过拟合误区一：由于自己炼丹次数太少，在训练模型这块没啥经验，一开始采用ResNet34，但是由于数据集太小，导致模型训练时准确率很高，但是在测试时很低。自己调试了很多参数都不行，查了一下，才想起来其实是过拟合了。更换了最简单５个卷积层，３个全连接的结构，效果有不小的提升。 pytorch的问题维度不一致在神经网络编程中，维度不一致是非常常见的问题。虽然pytorch相比tensorflow已经很好debug了，但是由于数据要转存到ＧＰＵ中，我在pycharm中实践去debug时，还是遇到很慢，而且也很不方便。百度查到了一个很好用的包:torchsummary1sudo pip install torchsummary 以查看我写的EasyNet网络为例：1234567891011121314from config import optimport osimport torch as tfrom models import EasyNetfrom torchsummary import summary# step1: load modelif not os.path.exists(opt.checkpoint_root): os.makedirs(opt.checkpoint_root)easynet = EasyNet()if opt.use_gpu: easynet = easynet.cuda()summary(easynet, (3, 224, 224)) 当网络出现维度问题，且我们查找问题并没有发现，并且数据集有很大，我们就可以加入这个包，summary(easynet, (3, 224, 224))：其实就是向网络加入一个batchsize=2的测试数据，我们就可以debug这句话，从而找到问题所处，还是比较方便。 pytorch中，将label变成one hot编码的形式这里参看了一篇非常优质的博客： Pytorch中，将label变成one hot编码的两种方式将一个mini batch的label向量变成形状为[batch size, class numbers]主要有两种实现方式：12tensor.scatter_tensor.index_select scatter_获得one hot 编码1234tensor.scatter_(dim, index, src)dim: 指明依托的维度index: 告诉将src中对应的值放在对应的那个位置src: 用来做替换的值，其中index的shape一定要和src保持一致 例子：123456789&gt;&gt;&gt; import torch&gt;&gt;&gt; x = torch.rand(2, 5)&gt;&gt;&gt; xtensor([[0.8443, 0.7421, 0.8100, 0.9166, 0.6908], [0.1860, 0.5211, 0.1552, 0.8594, 0.5856]])&gt;&gt;&gt; torch.zeros(3, 5).scatter_(0, torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)tensor([[0.8443, 0.5211, 0.1552, 0.9166, 0.6908], [0.0000, 0.7421, 0.0000, 0.8594, 0.0000], [0.1860, 0.0000, 0.8100, 0.0000, 0.5856]]) dim=0 代表第一个维度作为依托，index是一个二维数组[0,1,2,0,0][2,0,0,1,2]那我们可以得到要覆盖10个位置： 12[0,0];[1,1];[2,2];[0,3];[0,4][2,0];[0,1];[0,2];[1,3];[2,4] 我简单理解就是：由于dim=0, 所以index中每个位置的值都占最后得到的位置坐标的第一个位置，第二个维度的值从０以此增加到shape-1。这样，例如src中(1,2)的值替换数据的（１，１）的值。1234label # shape必须时２维label = t.LongTensor(label)y_one_hot = t.zeros(batch_size,class_num).scatter_(1,label,1)print(y_one_hot) tensor.index_select 获得one hot 编码123tensor.index_select( dim, index, out=None)dim : 指定按什么维度取tensor中的向量index : 是一个一维的张量。描述了按照dim维度取出tensor对应的index值的向量。 123456789ones = torch.sparse.torch.eye(class_num)# 生成class_num*class_num的单位矩阵# 例如：tensor([[1., 0., 0., 0.], # [0., 1., 0., 0.], # [0., 0., 1., 0.], # [0., 0., 0., 1.]]) ones.index_select(0,label) # label 必须是一维的，# dim=0：按行来选择,label中的值就指明了按将行中的哪一列设置为１ pytorch训练模型时代码结构 config：参数配置文件1234567891011121314151617class DefaultConfig(object): model = &apos;ResNet34&apos; # 使用的模型，名字必须与models/__init__.py中的名字一致 train_data_root = &apos;train_data/&apos; # 训练集存放路径 test_data_root = &apos;test_data/&apos; # 测试集存放路径 checkpoint_root = &apos;checkpoints/&apos; load_model_path = None # 加载预训练的模型的路径，为None代表不加载 batch_size = 64 # batch size use_gpu = True # user GPU or not num_workers = 4 # how many workers for loading data print_freq = 20 # print info every N batch max_epoch = 200 lr = 0.005 # initial learning rateopt = DefaultConfig() 这样修改模型参数时更方便 models:存放基础模型和自定义模型 basicmodule.py用来继承pytorch的nn.Module类的同时添加模型保存函数和模型加载函数123456789101112131415161718192021222324import torch as timport timeclass BasicModule(t.nn.Module): # 封装基础的nn.Module函数 def __int__(self): super(BasicModule, self).__init__() self.modal_name = str(type(self)) def load(self, path): # 加载指定路径的模型 self.load_state_dict(t.load(path)) def save(self, modalname, epoch=None): # 保存模型 prefix = &apos;checkpoints/&apos; + modalname + &apos;_&apos; + str(epoch) + &apos;_&apos; name = time.strftime(prefix + &apos;%m%d_%H:%M:%S.pth&apos;) t.save(self.state_dict(), name) return name def forward(self, *input): pass 自定义的函数这边就是定义自己的函数，注意一点就是新建的__init__.py文件中写入12from .resnet34 import ResNet34from .easynet import EasyNet 这样可以直接在main.py函数中直接引用from models import ResNet34 main主函数中定义train和test函数，为了方便执行，可以引入fire包123if __name__ == '__main__': import fire fire.Fire()]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>图像分类，pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu的小问题]]></title>
    <url>%2Fblog%2F2019%2F08%2F22%2Fubuntu%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言： 主要记录一些自己使用ubuntu18.04为主要操作系统中遇到的小问题，随时添加 删除问题 ubuntu中删除的命令是sudo rm，但是删除时要时刻注意自己要删除的文件中是否还有自己需要的，数据无价。 该命令中我们常用的参数有： １、sudo rm -rf : 删除目录以及子目录文件时，不进行删除询问，适合自己已经了解到了该文件夹的内容。 ２、 sudo rm -ri：删除时，进行是否删除的询问，在一些比较重要的文件中删除时，建议采用该命令 使用ubuntu商店下载软件时的问题 ubuntu18.04的商店已经有了丰富的软件，很多之前安装比较麻烦的软件都得到了很好的支持。有时安装一些软件时，会出现has install-snap change in progress，原因是之前安装过，但是没有安装好。解决办法：12snap changes #查看软件错误的ＩＤ，假设是１０sudo snap abort 10 ＃终止ID为10的。 vim基本快捷键学习前言：在训练模型时，有时候会用到云服务器，我一般采用终端进行控制，即没有图像界面，这样就会有个问题，当代码出现问题时，由于不会vim，就只能重新本地修改再上传，很麻烦。需要记住几个vim的简单快捷键，方便在终端做一些简单的代码修改。 首先输入vim 文件名即可进入vim编辑下 方向键：h：左移光标j： 下移光标k：上移光标l：右移光标 进入编辑模式：i：在光标前面进入插入I：在光标所在行的行首进入插入a：在光标后面进入插入A：在光标所在行的行尾进入插入o：在光标所在行的下方插入空行进入插入O：在光标所在行的上方插入空行进入插入s：删除光标指定的字符进入插入S：删除光标所在行并进入插入 按Esc退出插入模式 退出vim:q!：不保存直接退出:wq或者shitf+z+z即两个大写的z：保存退出 SSH的使用自己有台win10的笔记本和一台Ubuntu18.04的台式机，平常两台机器传送文件都是用U盘，很麻烦，现在换用SSH，以下场景是两台机器在同一路由下，即在同一局域网下。当二者不在局域网下时，还是采用teamviewer的文件传输功能。 安装ssh ubuntu直接在终端输入 1sudo apt install openssh-server win10就到openssh的官网下载安装即可 功能一 终端控制安装好之后，win10可以下载putty作为终端链接工具，在Ubuntu的终端下输入ifconfig，可能会提示没有该工具，安装即可。查看并记录自己Ubuntu的局域网内的ip，这个很关键。若想在win10通过终端控制ubuntu，打开putty，输入Ubuntu的ip即可。 功能二 文件传输在win10中，可以在cmd中输入1scp 文件名 ubuntu用户名@ip地址:存放的目录 但是Ubuntu通过该指令传到win10时，权限问题有点多，我不想搞，所以在win10安装了filezilla基于openssh的文件传输客户端。可以win10到Ubuntu相互传，就想teamviewer的文件传输功能。]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[奇异值分解]]></title>
    <url>%2Fblog%2F2019%2F08%2F17%2F%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[奇异值分解（SVD）的原理与在降维中的应用 总结原理并讨论在PCA降维算法中如何运用SVD 特征值和特征向量定义如下： $Ax=\lambda x$其中$A$是一个$n\times n$的矩阵，即是个方阵；x是一个n维向量。我们可以将矩阵A特征分解，如果我们求出来矩阵A的n个特征值，$\lambda_1 \leq \lambda_2\leq,\dots ,\lambda_n$以及这n个特征值所对应的特征向量$\{ w_1,w_2,\dots,w_n \}$如果这n个特征向量线性无关，则矩阵A就能对角化，表示为：$W^{-1}AW=\Sigma$所以得到$ A=W\Sigma W^{-1}$W是n个特征向量张成的$n\times n$维矩阵，$\Sigma$为主对角线为特征值的对角矩阵。一般会把W这n个特征向量标准化，即满足$||w_i||_2=1$,或者说$w_{i}^{T}w_i=1$,此时W的n个特征向量为标准正交基，满足$W^TW=I$，即$W^T=W^{-1}$，也就是W是酋矩阵 这样得到特征分解表达式： $A=W\Sigma W^T$ 注意：特征分解，矩阵A必须是方阵 SVD 奇异值分解如果A不是方阵，采用奇异值分解 假定矩阵A是一个$m\times n$的矩阵，定义矩阵A的SVD分解为：$A=U\Sigma V^T$其中U是一个$m\times n$的矩阵，$\Sigma$是一个$m\times n$的矩阵，是对角矩阵，即除对角线元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个$n\times n$的矩阵。U和V都是酋矩阵，即满足$U^TU=I,V^TV=I$。如果将A的转置和A做矩阵乘法，就会得到一个$n\times n$的方阵：$A^TA$。就可以对这个方阵做特征值分解，满足：$(A^TA)v_i=\lambda_iv_i$将$A^TA$的所有特征向量张成一个$n\times n$的酋矩阵V，一般称V的特征向量为右奇异值向量同理：将A和A的转置做矩阵乘法，得到一个$m\times m$的方阵：$AA^T$。其这个方阵进行特征分解，得到一个$m\times m$的酋矩阵U，称为左奇异值向量。 $A=U\Sigma V^T\Longrightarrow AV=U\Sigma V^TV \Longrightarrow AV = U\Sigma \Longrightarrow Av_i=\sigma_iu_i \Longrightarrow \sigma_i=\frac{Av_i}{u_i}$ 其实也是 $U^TAV=\Sigma$ 证明： 以V矩阵为例： $A=U\Sigma V^T \Longrightarrow A^T=V\Sigma ^TU^T \Longrightarrow A^TA=V\Sigma ^TU^TU\Sigma V^T=V\Sigma ^2V^T$ 其中：$U^TU=I,\Sigma ^T\Sigma=\Sigma^2$ $AA^T,A^TA$的特征值等于奇异值的平方。奇异值矩阵中，前10%或者1%的奇异值的和就占据了全部奇异值之和的99%。所以可以用k个奇异值和对应的左右奇异值向量来近似描述矩阵。]]></content>
      <categories>
        <category>优化求解</category>
      </categories>
      <tags>
        <tag>奇异值分解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-next博客搭建3-部署]]></title>
    <url>%2Fblog%2F2019%2F08%2F17%2Fhexo-next%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA3%2F</url>
    <content type="text"><![CDATA[当我们在本地运行好博客时，可以将它部署到免费的公共仓库来实现在线查看，主要的仓库网站有github和码云。github由于是美国网站，我们学校的网络push代码很慢，所以我选择了国内的网站码云。以下是码云的部署过程，github与之类似。 申请Gitee账号创建项目并设置首先新建仓库 然后进行如下设置 在站点配置文件_config.yml中配置Git1234deploy: type: git repo: https://gitee.com/QiangHaopeng/blog.git #上图中3的链接 branch: master 注意： 冒号后面要有空格 发布到Giteegit bash中输入1234npm install hexo-deployer-git --save #安装自动部署发布工具hexo cleanhexo g #生成hexo d #利用git将静态网站推送到Gitee的仓库中 首次发布需要输入Gitee的账号和密码 Gitee的pages服务开启选择仓库的服务 然后开启pages服务 首次是开启，得到得到自己的博客链接http://qianghaopeng.gitee.io/blog 在站点配置文件_config.yml中配置博客地址12url: http://qianghaopeng.gitee.io/blogroot: /blog 再执行命令：123hexo cleanhexo g #生成hexo d #利用git将静态网站推送到Gitee的仓库中 然后更新Gitee Pages过一段就能在自己的博客链接看到自己的博客主页了。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-next博客搭建2-设置]]></title>
    <url>%2Fblog%2F2019%2F08%2F17%2Fhexo-next%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA2%2F</url>
    <content type="text"><![CDATA[主要一些功能的实现，包括： 插入本地图像 latex公式显示 阅读时长加字数统计 代码块设置 打赏功能 评论功能 分享功能 等等 添加图像 首先站点配置文件里面的post_asset_folder设置为true 在git bash输入npm install hexo-asset-image --save 之后在生成博文的时候，默认也会在/source/_posts中生成一个同名的文件夹，把图片放到对应的文件夹下，采用1&#123;% asset_img 图像名 图像描述 %&#125; 第二个方案就是采用图床，可以用路过图床或者七牛，生成图像外链，采用markdown语法引用即可。 latex公式显示 更换hexo的渲染引擎 12npm uninstall hexo-renderer-markednpm install hexo-renderer-kramed --save 进入next的配置文件，打开mathjax开关，设置为true 解决语义冲突，在hexo的目录找到node_modules\kramed\lib\rules\inline.js，在该文件中找到escape和em字段进行如下修改：12escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 这样就可以使用latex公式了。 文章内超链接文本样式要求：超链接文本与普通文字区分在themes\next\source\css\_common\components\post\post.styl中最后部分添加如下代码：12345678910.post-body p a&#123;color: #0593d3; //原始链接颜色border-bottom: none;border-bottom: 1px solid #0593d3; //底部分割线颜色&amp;:hover &#123;color: #fc6423; //鼠标经过颜色border-bottom: none;border-bottom: 1px solid #fc6423; //底部分割线颜色&#125;&#125; 文章底部注释版权直接采用next的主题配置文件中，定位到creative_commons下的post，将false改成true 添加阅读时长和字数 安装hexo-symbols-count-time 1npm install hexo-symbols-count-time 配置hexo，在站点配置文件末尾添加： 12345symbols_count_time: symbols: true #字数 time: true #阅读时长 total_symbols: true # 所有文章总字数 total_time: true #总时长 配置next，修改主题配置设置文件： 123456789101112131415symbols_count_time: separated_meta: true item_text_post: true item_text_total: true awl: 4 wpm: 275 busuanzi_count: enable: true total_visitors: true total_visitors_icon: user total_views: true total_views_icon: eye post_views: true post_views_icon: eye 重启hexo服务 hexo首页文章显示一部分在文章中添加&lt;!--more--&gt;标记，只显示标记之前的内容。 代码块设置一共5种样式：normal，night，night blue， night eighties在主题配置文件中highlight_theme设置，同时设置复制按钮，在copy_button.enable中设置true，show_result中设置true 标签设置不喜欢#标签，可以设置tag_icon: true,还可以启用标签云, tagcloud为true 打赏将图片放在images，设置Reward中微信或者支付宝。 侧栏中显示进度条1234back2top: enable: true #打开后默认在右侧 # Back to top in sidebar. sidebar: true #打开后在侧栏 分享功能添加addthis，具体操作请参看博客(https://jasonssun.github.io/2019/06/15/Hexo搭建博客NexT主题之AddThis分享文章的配置/)，但是他的稍微有点问题，最后的配置规则部分可以不设置，得到了代码配置就好了，若设置规则，可能会有问题。 评论功能这里我使用的来比力，进入该网站注册账号，登录后选择上方关键字安装，选择免费版本，然后会得到来比力city版安装代码在主题配置文件中查找关键字livere_uid，将data-uid等号后面，引号之中的内容复制到过来，注意如果有==，要删除。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>功能实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+next博客搭建-安装]]></title>
    <url>%2Fblog%2F2019%2F08%2F16%2Fhexo-next%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA1%2F</url>
    <content type="text"><![CDATA[前言在一天逛B站的时候，看到了羊哥（codesheep）发布的“手把手教你从0开始搭建自己的个人博客 |无坑版视频教程| hexo”视频，链接：https://www.bilibili.com/video/av44544186?t=583 。就根据视频进行了博客系统的搭建。 经过查阅其他的人的博客系统以及自己的摸索，自己使用hexo+next来搭建。也可以根据Next官方文档配置（https://theme-next.org/docs/getting-started/） 安装nodejs 和 git windows10可以直接在nodejs官方下载，以及git工具。如图 Ubuntu18.04可以直接sudo apt install nodejs来下载(ubuntu配置博客功能时有问题，我没时间再搞，以下主要是win10平台) 安装好之后，新建一个文件夹专门用来存放自己的blog系统，这样即使自己搞坏了博客系统配置，也可以删除该文件夹，重新配置blog，丝毫不会影响电脑中的其他文件。例如我新建了E:/MyBlog文件，进入文件右键点击 git bash here, 进入控制台，可以输入node -v和npm -v来查看node版本，可以时常通过pwd来确定自己所在路径，确保在MyBlog目录下。 更换淘宝源直接采用npm下载很慢，我们通过换用淘宝源来下载hexo代码： 1npm install -g cnpm --registry=https://registry.npm.taobao.org 安装完成可利用cnpm -v查看版本号输入cnpm install -g hexo-cli来安装hexo，完成之后hexo -v检查版本号 初始化blog系统在MyBlog文件夹下输入hexo init来初始化博客系统。 遇到的问题：git clone特别慢 解决办法：修改hosts文件。windows下：用编辑器打开host文件： C:\Windows\System32\drivers\etc\hosts， 在hosts文件末尾添加下面两个地址; Ubuntu下同理，sudo gedit /etc/hosts打开hosts，添加下面两个地址，sudo /etc/init.d/networking restart重新载入hosts12151.101.72.249 github.global.ssl.fastly.net192.30.253.112 github.com 从而初始化得到博客系统。以下一些在git bash中操作指令123hexo clean #清理hexo中生成的博客文件，用来初始化一下hexo g #生成博客文件hexo s #开启博客服务，本地可以在`localhost:4000`来访问，`ctrl+c`终止博客服务。 特别提醒：Ubuntu尽可能在管理员下操作，sudo su从而保证权限。 配置主题在MyBlog文件夹下的git bash输入git clone https://github.com/theme-next/hexo-theme-next themes/next，从而clone下来next主题文件。我们的优化都是基于next主题进行的。首先在站点配置文件MyBlog/_config.yml设置，可以利用nodepad++或者其他开发工具打开该配置文件，进行如下设置 然后配置主题配置themes/next/_config.yml: 修改站点图标 可以自己找的图像ps成16*16，32*32各1张，放到路径themes/next/source/images下。特别注意涉及到主题一些图像的变换，一般都是放到主题下面的这个文件夹中，在配置文件的路径填写时下/images/图像名即可 个人链接 个人的一些社交媒体的账号展示 个人头像 同理，将自己的头像放到next/source/images下. 主页显示的标识 主页显示的标识。同时，next下有四个主题，在172到176行可以选择，我选择的第四个，所以直接把注释#去掉即可。 效果展示 此时点击关于、标签、归类时，显示是空界面，因为我们没创建。 bash或者git bash执行：hexo new page categories，在MyBlog的文件source就会多出来一个categories文件夹，打开该文件夹，其中的index.md写入： 12345---title: 分类date: 2019-08-16 18:11:08type: &quot;categories&quot;--- 三个短线表示md文件的标记，其中日期不用管，主要更改title，以及type，将其值设置为”categories”，注意，冒号之后都要有空格。 同样方法更改bags：hexo new page tags，type设置为tags。 特别注意：此后写的博客中的标签和分类都会自动归档到分类和标签中。但是要在博客的md文件中声明分类和标签。每次都去写声明很麻烦，我们可以在MyBlog/scaffolds/post.md中设置好模板。输入： 123456---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:categories:--- 写第一篇博客在MgBlog文件夹下的git bash 运行hexo n 博客名 我们的博客的md文件其实都存放在Myblog/source/_posts中，我们在刚生成的md文件中书写博客，运行hexo g和hexo s就可以在本地的浏览器localhost:4000中查看。如果我们不想要这篇博客了，直接删除该文件夹中该博客的.md文件即可。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>nodejs</tag>
        <tag>gitclone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word Embedding介绍]]></title>
    <url>%2Fblog%2F2019%2F08%2F15%2Fembedding%2F</url>
    <content type="text"><![CDATA[文本特征选择就是在训练集中选择一个词汇子集的过程。 现在的方法不能直接处理文本数据，通过Word Embedding的方法将文本数据转化为数值型数据。如果将word看作文本的最小单元，可以将Word Embedding理解为一种映射，或者说是嵌入（embedding）到一个数值向量空间。之所以称之为Embedding，因为这种方法往往伴随着一种降维的思想。 就是把one-hot编码的词嵌入到一个低维空间。 常用的文本特征选择方法有 词频法 TF 词频逆文档频率法 TF-IDF 信息增益 IG 互信息 MI TF 和 IDF都是在不考虑分类分布的情况下消除低频词 Word Embedding的输入Word Embedding的输入是原始文本中一组不重叠的词汇，假设有句子：apple on a apple tree 为了方便处理，把这些词汇放在一个dictionary：[apple, on, a, tree] ，这个dictionary可以看做wordEmbedding的输入 Word Embedding的输出就是每一word的向量表示，最简单的方式就是one-hot编码方式，那么每一word都对应了一种数值表示。 apple对应的vector就是[1,0,0,0] a对应的是[0,0,1,0] Word Embedding的类型主要有两种： 基于频率 基于预测 基于频率的Word Embedding基于频率的Word Embedding又可以分为： Count Vector TF—IDF Vector Co-Occurence Vector 其本质都是基于one-hot，以频率为主的加权。 Count Vector假设有一个语料库C，其中有D个文档：$d_1,d_2,\dots,d_D$,$C$中一共有$N$个word，这$N$个word构成原始输入的dictionary，据此可以生成一个矩阵M，其规模为$D\times N$ 例子： 语料库内容如下： D1：He is a boy D2：She is a girl，good girl 那么可以构造一个$2\times 7$的矩阵 He She is a boy girl good D1 1 0 1 1 1 0 0 D2 0 1 1 1 0 2 1 每个文档用词向量来表示，那么dictionary很庞大，矩阵是稀疏的，大量的无用信息。通常的做法是选取出现次数最频繁的那些词来构建dictionary，例如top10000 TF-IDF上面的只考虑到了词频TF，也就是词在文档中出现的频率。TF越大，说明词在本文档中的重要性越高，对应的权重也就越高。这个思路大体上来说是对的，例如，对于一个主题是Cat的文档，显然Cat这个词汇在本文档中的出现频率会相对高。 但如果我们把视野扩展到整个语料库，会发现，像is，a等通用词汇，几乎在每个文档里出现的频率都很高。由此，我们可以得到这样的结论：对于一个word，如果在特定文档里出现的频率高，而在整个语料库里出现的频率低，那么这个word对于该文档的重要性就比较高。因此我们可以引入逆文档频率IDF（Inverse Document Frequency）的概念： ​ $IDF = log(N/n)$ N代表语料库中文档的总数，n代表某个word在n个文档中出现过。 因此当一个word出现的频繁，那么IDF就越小。IDF用来惩罚常用词汇，TF用来奖励那么特定文档中出现频繁的词汇。用$TF \times IDF$来表示词汇的权重，这样可以提取出来文档的关键词。 对于一个word， 如果在特定文档里出现的频率高，即TF较大，则反映出了该word在自己的文档中可能是比较关键的词。 在整个语料库里出现的频率低，即IDF较小，则排除了常用词的干扰。 例子： 语料库共有2个文档，其中一个文档名为d，且文档d一共有8个词汇。 cat只出现在文档d中，出现过4次； is在两个文档中都有出现，在d中出现了4次。 那么对于文档d进行提取关键词： $TF(‘cat’, d) = \frac{4}{8} = 0.5$ $TF(‘is’, d) = \frac{4}{8} = 0.5$ $IDF(‘cat’, d) = log(\frac21) = 0.301$ $IDF(‘is’, d) = log(\frac22) = 0$ 那么计算TF-IDF $TFIDF(‘cat’, d) = 0.5 \times 0.301 = 0.15$ $TFIDF(‘is’, d) = 0$ 这样就能找出来文件的关键词汇。 Co-Occurence Vector自然语言一大特色是语义和上下文。有如下著名的研究结果：相似的单词趋向于有相似的上下文(context)。举例： 那个人是个男孩 那个人是个女孩 Context Window 上下文窗口，需要定义其长度。例如： 定义窗口大小为2，则such的Context Window如图所示。 Co-Occurence（共现） 对于such这个单词来说，在其上下文窗口内，它分别与[she, is, a, beautiful]这四个单词各出现了一次共现。如果我们在语料库中所有such出现的地方，计算其共现的单词，并按次数累加，那么我们就可以利用其上下文范围内的单词来表示such这个词，这就是Co-Occurence Vector的计算方法。 假定有如下语料库： He is not lazy. He is intelligent. He is smart. 当窗口大小为2的时候，可以得到如下共现矩阵： 计算过程如图所示： 这种方法可以保留语义信息。 基于预测的Word Embedding算法满足 携带上下文信息 词的表示是稠密的 算法主要有CBOW和Skip-Gram CBOW （continues bag of words）本质是通过context来预测word 如图所示： 首先语料库内的每一word都可以用one-hot编码，假设选取context window为2，那么模型中的一对input和target就是： input：He 和 is 的one-hot编码 target：a 的 one-hot编码 算法步骤： 窗口大小为C/2，W：$V\times N$， $V =( \sum_{i=0}^{C} W\times C_i)/C$ 最终网络输出层才用softmax，即计算概率，与a 的one-hot编码作对比。该方法关注点在于hidden layer的输出，因此word是one-hot编码， 1234import tensorflow as tfhello = tf.constance('Hello Google!')sess = tf.Session()print(sess.run(hello))]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2019%2F08%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
